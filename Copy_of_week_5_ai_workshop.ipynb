{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/np03cs4a240245-cpu/ai-rakesh/blob/main/Copy_of_week_5_ai_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zBF-nP7tDCEM"
      },
      "outputs": [],
      "source": [
        "def cost_function(X, Y, W):\n",
        "    \"\"\" Parameters:\n",
        "    This function finds the Mean Square Error.\n",
        "    Input parameters:\n",
        "    X: Feature Matrix\n",
        "    Y: Target Matrix\n",
        "    W: Weight Matrix\n",
        "    Output Parameters:\n",
        "    cost: accumulated mean square error.\n",
        "    \"\"\"\n",
        "    # Your code here:\n",
        "    m=len(Y)\n",
        "\n",
        "    j = np.sum((X.dot(W)-Y)** 2)/(2*m)\n",
        "\n",
        "    return j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Test data\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "# Compute cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "# Check the cost\n",
        "if np.isclose(cost, 0):\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "    print(\"Cost function output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UjCFykAGyp3",
        "outputId": "72b0a4f2-5f36-4ffa-d5e5-c7328b43e420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XxDWm06FCdyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2fkHgKcjDcWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "    Returns:\n",
        "\n",
        "\n",
        "    tuple: A tuple containing the final optimized parameters (W) and the history of cost values\n",
        "    .\n",
        "    W (numpy.ndarray): Updated parameters (n x 1).\n",
        "    cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = X.dot(W)\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (X.T.dot(loss)) / m\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W = W - alpha * dw\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W, cost_history"
      ],
      "metadata": {
        "id": "ryjBUyZ2EBkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate random test data\n",
        "np.random.seed(0) # For reproducibility\n",
        "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "id": "a7D74wyFLOO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Squres.\n",
        "    Input Arguments:\n",
        "    Y: Array of actual(Target) Dependent Varaibles.\n",
        "    Y_pred: Array of predeicted Dependent Varaibles.\n",
        "    Output Arguments:\n",
        "    rmse: Root Mean Square.\n",
        "    \"\"\"\n",
        "    # Your Code Here\n",
        "    rmse = np.sqrt(np.mean ((Y - Y_pred )** 2))\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "K_EibjxkLO0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the R Squared score.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (target) dependent variables\n",
        "    Y_pred: Array of predicted dependent variables\n",
        "\n",
        "    Output:\n",
        "    r2: R Squared score\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "\n",
        "    # Total sum of squares\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Residual sum of squares\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # R-squared calculation\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "EatyyGfSAIbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Bishal AI Nasty/student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values   # Features\n",
        "    Y = data['Writing'].values             # Target\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, Y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 4: Initialize weights, learning rate, and iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.00001                 # Learning rate\n",
        "    iterations = 1000               # Number of iterations\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(\n",
        "        X_train, Y_train, W, alpha, iterations\n",
        "    )\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QJvdFOAOAkLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSgYIOd90hOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}